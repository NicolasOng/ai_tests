{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicolas Ong\\Documents\\Data2\\Python\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 SUPER'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at all the envs available\n",
    "def list_gym_environments():\n",
    "    print(\"num envs: \" + str(len(gym.envs.registry)))\n",
    "    for k, v in gym.envs.registry.items():\n",
    "        print(k)\n",
    "\n",
    "list_gym_environments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=False, render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   #print(env.action_space.sample())\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   #print(observation)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "      break\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a DQN (failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, batch_size):\n",
    "        self.buffer = deque(maxlen=1000)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        return random.choices(self.buffer, k=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, h1_dim, h2_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, h1_dim)\n",
    "        self.linear2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.linear3 = nn.Linear(h2_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.replayBuffer = ReplayBuffer(50000)\n",
    "\n",
    "        self.qNetwork = QNetwork(8, 16, 16, 4).to(device)\n",
    "        self.targetNetwork = QNetwork(8, 16, 16, 4).to(device)\n",
    "        self.targetNetwork.eval()\n",
    "        self.updateTargetNet()\n",
    "\n",
    "        self.training = True\n",
    "        self.epsilon = 1\n",
    "        self.gamma = 0.95\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.qNetwork.parameters(), lr=0.00025)\n",
    "    \n",
    "    def updateTargetNet(self):\n",
    "        self.targetNetwork.load_state_dict(self.qNetwork.state_dict())\n",
    "    \n",
    "    def set_training(self, train):\n",
    "        self.training = train\n",
    "    \n",
    "    def get_action(self, state, action_space):\n",
    "        if (self.training and np.random.random() < self.epsilon):\n",
    "            action = action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.qNetwork(torch.tensor(np.array(state)).to(device)).max(0, keepdim=True)[1].item()\n",
    "        return action\n",
    "    \n",
    "    def record_experience(self, state, action, reward, next_state, done):\n",
    "        self.replayBuffer.add_experience(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train(self, iteration):\n",
    "        # get the batch\n",
    "        batch = self.replayBuffer.sample_batch()\n",
    "        states, actions, rewards, next_states, done = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64).unsqueeze(-1).to(device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "        done = torch.tensor(np.array(done), dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        \n",
    "        #Calculate the targets using: r + γ * max_a' Q(s', a'; θ^-)\n",
    "        next_q_values = self.targetNetwork(next_states).max(1, keepdim=True)[0].detach()\n",
    "        targets = rewards + (1 - done) * self.gamma * next_q_values\n",
    "\n",
    "        #Calculate the loss with: L(θ) = E[(Q(s, a; θ) - (r + γ * max_a' Q(s', a'; θ^-))^2]\n",
    "        loss = self.loss_fn(self.qNetwork(states), targets)\n",
    "\n",
    "        #optmize:\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #update target network periodically\n",
    "        if (iteration % 1000 == 0):\n",
    "            self.updateTargetNet()\n",
    "        \n",
    "        #ε = ε_min + (ε_max - ε_min) * exp(-decay_rate * step)\n",
    "        epsilon_min = 0.01\n",
    "        self.epsilon = epsilon_min + (1 - epsilon_min) * math.exp(-0.001 * iteration)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.qNetwork.state_dict(), \"2023-04-08 model.pth\")\n",
    "    \n",
    "    def load(self):\n",
    "        self.qNetwork.load_state_dict(torch.load(\"2023-04-08 model.pth\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DQN (failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=False)\n",
    "\n",
    "state, info = env.reset(seed=42)\n",
    "cur_reward = 0\n",
    "rewards = []\n",
    "for i in range(10000):\n",
    "   action = agent.get_action(state, env.action_space)\n",
    "   next_state, reward, terminated, truncated, info = env.step(action)\n",
    "   agent.record_experience(state, action, reward, next_state, terminated or truncated)\n",
    "   cur_reward += reward\n",
    "\n",
    "   state = next_state\n",
    "\n",
    "   agent.train(i)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      print(i, cur_reward)\n",
    "      rewards.append(cur_reward)\n",
    "      cur_reward = 0\n",
    "      state, info = env.reset()\n",
    "      #break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(10):\n",
    "   #print(env.action_space.sample())\n",
    "   action = env.action_space.sample()  # int [0, 3]\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   print(observation, reward) # observation: int [0, 15] (player's position on the 4x4)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "      #break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning\n",
    "class ActionValueFunction():\n",
    "    def __init__(self, s, a):\n",
    "        self.sa_pairs = np.zeros((s, a))\n",
    "        #self.sa_pairs = np.full((s, a), 100)\n",
    "    \n",
    "    def get_action_value(self, s, a):\n",
    "        return self.sa_pairs[s][a]\n",
    "    \n",
    "    def set_action_value(self, s, a, v):\n",
    "        self.sa_pairs[s][a] = v\n",
    "    \n",
    "    def get_best_action(self, s):\n",
    "        #returns the index of the action with the highest action-value, given the state.\n",
    "        return np.argmax(self.sa_pairs[s])\n",
    "    \n",
    "    def get_best_action_value(self, s):\n",
    "        #returns the value of the action with the highest action-value, given the state.\n",
    "        return np.max(self.sa_pairs[s])\n",
    "    \n",
    "    def load(self, sa):\n",
    "        self.sa_pairs = sa\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.q = ActionValueFunction(16, 4)\n",
    "        self.epsilon = 1\n",
    "        self.alpha = 1 #learning rate\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if (np.random.random() < self.epsilon):\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            action = self.q.get_best_action(state)\n",
    "            #print(\"The action-values for this state: \", self.q.sa_pairs[state], state)\n",
    "            #print(\"I choose the action: \", action)\n",
    "        return action\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(0.1, self.epsilon * 0.99) # Decay factor can be adjusted\n",
    "\n",
    "    def explore(self):\n",
    "        self.epsilon = 1\n",
    "\n",
    "    def exploit(self):\n",
    "        self.epsilon = 0\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        current_action_value = self.q.get_action_value(state, action)\n",
    "        max_next_action_value = self.q.get_best_action_value(next_state)\n",
    "        updated_action_value = current_action_value + self.alpha * ((reward + (self.gamma * max_next_action_value)) - current_action_value)\n",
    "        self.q.set_action_value(state, action, updated_action_value)\n",
    "        #print(current_action_value, updated_action_value)\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.q.sa_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the action-value function Q(s, a) for all state-action pairs to zero or random small numbers\n",
    "agent = Agent()\n",
    "agent.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=None)\n",
    "state, info = env.reset(seed=42)\n",
    "\n",
    "#agent.exploit()\n",
    "\n",
    "for i in range(10000):\n",
    "    # Choose an action (a) for the current state (s) using an exploration strategy (e.g., ε-greedy).\n",
    "    action = agent.get_action(state)\n",
    "\n",
    "    # Take action (a), observe the next state (s') and reward (r).\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    #Update the action-value function Q(s,a) using the observed transition and the Bellman optimality equation\n",
    "    #Q(s,a) <- Q(s,a) + α * (r + γ * max_a'[Q(s',a')] - Q(s,a)) where α is the learning rate (0 < α ≤ 1).\n",
    "    agent.update(state, action, reward, next_state)\n",
    "\n",
    "    # Set the current state (s) to the next state (s') and repeat steps b to d until a stopping condition is met (e.g., a certain number of episodes or a convergence criterion).\n",
    "    state = next_state\n",
    "\n",
    "    #clear_output(wait=True)\n",
    "    #print(i)\n",
    "    #print(agent.q.sa_pairs)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        state, info = env.reset()\n",
    "        reward = 0\n",
    "        #agent.decay_epsilon()\n",
    "        #break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_sa = agent.q.sa_pairs\n",
    "print(saved_sa)\n",
    "#np.savetxt('./weights/2023-04-09 01 solved_frozenlake_42.txt', saved_sa, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_array = np.loadtxt('./weights/2023-04-09 01 solved_frozenlake_42.txt', delimiter=',')\n",
    "print(loaded_array)\n",
    "agent.q.load(loaded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data into a 4x4 grid\n",
    "grid = saved_sa.reshape(4, 4, 4)\n",
    "\n",
    "# Display each 4-dimensional array as a square in a 4x4 grid\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(grid[i][j], end=\" | \")\n",
    "    print(\"\\n\" + \"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = env.desc\n",
    "\n",
    "# Print the layout\n",
    "print(layout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a DQN Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "state, info = env.reset(seed=42)\n",
    "for _ in range(50):\n",
    "   #print(env.action_space.sample()) # int [0, 1]\n",
    "   action = env.action_space.sample()\n",
    "   new_state, reward, terminated, truncated, info = env.step(action)\n",
    "   #print(new_state, reward) # 4 floats, int\n",
    "\n",
    "   state = new_state\n",
    "\n",
    "   if terminated or truncated:\n",
    "      state, info = env.reset()\n",
    "      reward = 0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_dim=4, h_dim=10, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_len, batch_size):\n",
    "        self.buffer = deque(maxlen=max_len)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        return random.sample(self.buffer, k=self.batch_size)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.replayBuffer = ReplayBuffer(50000, 64)\n",
    "\n",
    "        self.qNetwork = QNetwork().to(device)\n",
    "        self.targetNetwork = QNetwork().to(device)\n",
    "        self.updateTargetNet()\n",
    "        self.target_network_update_period = 1000\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = False\n",
    "        self.gamma = 0.95\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.qNetwork.parameters(), lr=0.00025)\n",
    "    \n",
    "    def updateTargetNet(self):\n",
    "        self.targetNetwork.load_state_dict(self.qNetwork.state_dict())\n",
    "    \n",
    "    def get_action(self, state, action_space):\n",
    "        if (np.random.random() < self.epsilon):\n",
    "            action = action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #convert state list to tensor\n",
    "                state_tensor = torch.tensor(np.array(state), dtype=torch.float32).to(device)\n",
    "                #get the estimated q-values given the state\n",
    "                q_values = self.qNetwork(state_tensor)\n",
    "                #get the action with the highest value (getting the action/index, not the value)\n",
    "                action = q_values.max(0, keepdim=True)[1].item()\n",
    "        return action\n",
    "    \n",
    "    def record_experience(self, state, action, reward, next_state, done):\n",
    "        self.replayBuffer.add_experience(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train(self, iteration):\n",
    "        # get the batch\n",
    "        batch = self.replayBuffer.sample_batch()\n",
    "        # (maybe I should hide all this in the replay buffer?)\n",
    "        states, actions, rewards, next_states, done = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64).unsqueeze(-1).to(device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.int64).unsqueeze(-1).to(device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "        done = torch.tensor(np.array(done), dtype=torch.float32).unsqueeze(-1).to(device) # False = 0, True = 1\n",
    "        \n",
    "        #Calculate the targets using: r + γ * max_a' Q(s', a'; θ^-)\n",
    "        next_q_values = self.targetNetwork(next_states).max(1, keepdim=True)[0].detach() #detach so the backprop doesn't go into the target network\n",
    "        targets = rewards + (1 - done) * self.gamma * next_q_values\n",
    "\n",
    "        #Calculate the loss with: L(θ) = E[(Q(s, a; θ) - (r + γ * max_a' Q(s', a'; θ^-))^2]\n",
    "        current_q_values = self.qNetwork(states).max(1, keepdim=True)[0] # had to fix this (maybe I should hide this line in a new \"Q-Value\" class?)\n",
    "        loss = self.loss_fn(current_q_values, targets)\n",
    "\n",
    "        #optmize:\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #update target network periodically\n",
    "        if (iteration % self.target_network_update_period == 0):\n",
    "            self.updateTargetNet()\n",
    "        \n",
    "        #ε = ε_min + (ε_max - ε_min) * exp(-decay_rate * step)\n",
    "        if (self.epsilon_decay):\n",
    "            epsilon_min = 0.01\n",
    "            self.epsilon = epsilon_min + (1 - epsilon_min) * math.exp(-0.001 * iteration)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.qNetwork.state_dict(), \"2023-04-09 model.pth\")\n",
    "    \n",
    "    def load(self):\n",
    "        self.qNetwork.load_state_dict(torch.load(\"2023-04-09 model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things to monitor during training: loss function - does it go down?. estimated Q-values in different areas of the observation space - are they moving as expected?. Agent performance/how long it lasts in the arena (during not-training)\n",
    "#Idea: random actions for a while, without training (the replay buffer needs to stock up). Then training with an epsilon decay (random at first, starts to further explore the better areas). Terminate on a condition (q-value estimation stabalizes? num times? agent performance perfect?). Measure performance in full exploitation mode.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
