{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 SUPER'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "#The dataset is in PIL, needs to be Tensors.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('27463172138697354961', '1144367217')\n",
      "('11812419430958682846', '2139924789')\n",
      "('31631197858963334018', '1212645380')\n"
     ]
    }
   ],
   "source": [
    "def format_integer(number):\n",
    "    str_len = 10\n",
    "    number_str = str(number)\n",
    "    if len(number_str) > str_len:\n",
    "        number_str = number_str[:str_len]\n",
    "    elif len(number_str) < str_len:\n",
    "        number_str = number_str.zfill(str_len)\n",
    "    return number_str\n",
    "\n",
    "def generate_sample():\n",
    "    start, end = 0, 9999999999\n",
    "    a = random.randint(start, end)\n",
    "    b = random.randint(start, end)\n",
    "    return format_integer(a) + format_integer(b), format_integer(a + b) # both should be pytorch tensors. floats are too small to hold the number.\n",
    "    # actually, label being a torch.double is fine, as long as it's < 9007199254740992! (double)\n",
    "\n",
    "#generate the dataset\n",
    "dataset_size = 10000\n",
    "train_percent = 0.8\n",
    "train_size = math.floor(dataset_size * train_percent)\n",
    "test_size = math.floor(dataset_size * (1 - train_percent))\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# I'll start with random, then try other things. some ideas:\n",
    "# - keep the format, but only generate training data with 5-digit ints. see if the model can generalize.\n",
    "# - try generating a training set with only 5-digit ints, but most of the training data is for below 3digits. Does this help or hurt generalization?\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    train_data.append(generate_sample())\n",
    "\n",
    "for i in range(test_size):\n",
    "    test_data.append(generate_sample())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s = self.samples[index]\n",
    "        return s.input, s.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999999980000000001\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #define the layers here\n",
    "        self.fn1 = nn.Linear(20, 200)\n",
    "        self.fn2 = nn.Linear(200, 50)\n",
    "        self.fn3 = nn.Linear(50, 20)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #defined the forward pass here\n",
    "        x = F.relu(self.fn1(x))\n",
    "        x = F.relu(self.fn2(x))\n",
    "        x = self.fn3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LinearNet()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = count_parameters(net)\n",
    "print(f\"Total number of parameters: {total_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        cur_iter += 1\n",
    "        x_batch.append(cur_iter)\n",
    "\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss and accuracy for the current batch\n",
    "        batch_loss = loss.item()\n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {batch_loss :.3f}')\n",
    "        training_loss_list.append(batch_loss)\n",
    "        batch_acc = (torch.argmax(outputs, dim=1) == labels).sum().item() / 64\n",
    "        train_acc_list.append(batch_acc)\n",
    "\n",
    "        #validation loss and model accuracy for the entire test dataset - every 100 batches\n",
    "        if (i % 500 == 499):\n",
    "            x_batch_500.append(cur_iter)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            running_loss = 0\n",
    "            iterations = 0\n",
    "            with torch.no_grad():\n",
    "                for data in test_dataloader:\n",
    "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    running_loss += criterion(outputs, labels)\n",
    "                    iterations += 1\n",
    "\n",
    "                    total += labels.size(0)\n",
    "                    correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "            test_acc = correct/total\n",
    "            test_acc_list.append(test_acc)\n",
    "            test_loss = running_loss/iterations\n",
    "            test_loss_list.append(test_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
